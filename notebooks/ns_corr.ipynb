{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759aeef2-7ec6-4c2c-915e-7980c4f86380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/zwang34/miniconda3/envs/ibl-fm/lib/python310.zip', '/home/zwang34/miniconda3/envs/ibl-fm/lib/python3.10', '/home/zwang34/miniconda3/envs/ibl-fm/lib/python3.10/lib-dynload', '', '/home/zwang34/miniconda3/envs/ibl-fm/lib/python3.10/site-packages', './src']\n"
     ]
    }
   ],
   "source": [
    "# Path\n",
    "import os, sys\n",
    "os.chdir('/home/zwang34/IBL/iblfm_exp/IBL_foundation_model')\n",
    "sys.path.append('./src')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04f8744-7774-4937-8081-a476561dbc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zwang34/miniconda3/envs/ibl-fm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Lib\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import numpy as np\n",
    "from accelerate import Accelerator\n",
    "from loader.make_loader import make_loader\n",
    "from utils.eval_utils import bits_per_spike\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.utils.utils import move_batch_to_device, metrics_list, plot_gt_pred, plot_neurons_r2\n",
    "import wandb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcb27df4-0bdc-4f4d-92ae-81a6c97dd466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args\n",
    "non_randomized = False\n",
    "unaligned = False\n",
    "eid = '746d1902-fa59-4cab-b0aa-013be36060d5'\n",
    "sigma = 200  # For smoothing\n",
    "seed = 12  # For the random signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f281ee7-83ef-4a82-9a38-bc5f7b12caf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 2.54k/2.54k [00:00<00:00, 12.7MB/s]\n",
      "Downloading data: 100%|██████████| 17.4M/17.4M [00:00<00:00, 71.8MB/s]\n",
      "Downloading data: 100%|██████████| 2.74M/2.74M [00:00<00:00, 25.7MB/s]\n",
      "Downloading data: 100%|██████████| 5.19M/5.19M [00:00<00:00, 42.6MB/s]\n",
      "Generating train split: 100%|██████████| 453/453 [00:00<00:00, 640.77 examples/s]\n",
      "Generating val split: 100%|██████████| 65/65 [00:00<00:00, 590.83 examples/s]\n",
      "Generating test split: 100%|██████████| 130/130 [00:00<00:00, 657.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spikes_sparse_data', 'spikes_sparse_indices', 'spikes_sparse_indptr', 'spikes_sparse_shape', 'choice', 'reward', 'block', 'whisker-motion-energy', 'binsize', 'interval_len', 'eid', 'sampling_freq', 'cluster_regions', 'cluster_channels', 'cluster_depths', 'good_clusters', 'cluster_uuids', 'cluster_qc', 'start_times', 'end_times']\n",
      "n neurons: 1337\n",
      "len(dataset): 453\n"
     ]
    }
   ],
   "source": [
    "# Dataset and dataloader\n",
    "if unaligned:\n",
    "    _al = load_dataset(f'neurofm123/{eid}_aligned', cache_dir='/expanse/lustre/scratch/zwang34/temp_project/iTransformer/checkpoints/datasets_cache', download_mode='force_redownload')\n",
    "    _ual = load_dataset(f'neurofm123/{eid}', cache_dir='/expanse/lustre/scratch/zwang34/temp_project/iTransformer/checkpoints/datasets_cache', download_mode='force_redownload')\n",
    "    dataset = split_unaligned_dataset(_al, _ual)\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    val_dataset = dataset[\"val\"]\n",
    "    test_dataset = dataset[\"test\"]    \n",
    "elif non_randomized:\n",
    "    dataset = load_dataset(f'neurofm123/{eid}_nonrandomized', cache_dir='/expanse/lustre/scratch/zwang34/temp_project/iTransformer/checkpoints/datasets_cache', download_mode='force_redownload')\n",
    "    train_dataset = dataset['train']\n",
    "    val_dataset = dataset['val']\n",
    "    test_dataset = dataset['test']\n",
    "else:\n",
    "    dataset = load_dataset(f'neurofm123/{eid}_aligned', cache_dir='/expanse/lustre/scratch/zwang34/temp_project/iTransformer/checkpoints/datasets_cache', download_mode='force_redownload')\n",
    "    train_dataset = dataset['train']\n",
    "    val_dataset = dataset['val']\n",
    "    test_dataset = dataset['test']\n",
    "\n",
    "try:\n",
    "    bin_size = train_dataset[\"binsize\"][0]\n",
    "except:\n",
    "    bin_size = train_dataset[\"bin_size\"][0]\n",
    "\n",
    "print(train_dataset.column_names)\n",
    "whole_dataset = concatenate_datasets([train_dataset, val_dataset, test_dataset])\n",
    "max_time = int(max(whole_dataset['start_times']))\n",
    "\n",
    "n_neurons = len(train_dataset[0]['cluster_uuids'])\n",
    "print(f'n neurons: {n_neurons}')\n",
    "\n",
    "train_dataloader = make_loader(\n",
    "    train_dataset,\n",
    "    target='start_times_raw',\n",
    "    load_meta=True,\n",
    "    batch_size=16,\n",
    "    pad_to_right=True,\n",
    "    pad_value=-1,\n",
    "    bin_size=0.02,\n",
    "    max_time_length=100,\n",
    "    max_space_length=n_neurons,\n",
    "    dataset_name='ibl',\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_dataloader = make_loader(\n",
    "    val_dataset,\n",
    "    target='start_times_raw',\n",
    "    load_meta=True,\n",
    "    batch_size=10000,\n",
    "    pad_to_right=True,\n",
    "    pad_value=-1,\n",
    "    bin_size=0.02,\n",
    "    max_time_length=100,\n",
    "    max_space_length=n_neurons,\n",
    "    dataset_name='ibl',\n",
    ")\n",
    "\n",
    "test_dataloader = make_loader(\n",
    "    test_dataset,\n",
    "    target='start_times_raw',\n",
    "    load_meta=True,\n",
    "    batch_size=10000,\n",
    "    pad_to_right=True,\n",
    "    pad_value=-1,\n",
    "    bin_size=0.02,\n",
    "    max_time_length=100,\n",
    "    max_space_length=n_neurons,\n",
    "    dataset_name='ibl',\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67870ba-5ede-466f-94e4-4fd6fae5d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random smooth signal\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "def generate_smooth_random_time_series(time_array, sigma=500, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    random_values = np.random.rand(time_array.shape[0])\n",
    "\n",
    "    smooth_values = gaussian_filter1d(random_values, sigma=sigma)\n",
    "    smooth_stand_values = (smooth_values - smooth_values.mean()) / smooth_values.std()\n",
    "\n",
    "    return smooth_stand_values\n",
    "\n",
    "\n",
    "time_array = np.linspace(0, 10, max_time+2)\n",
    "smooth_random_series = generate_smooth_random_time_series(time_array, sigma=sigma, seed=seed)\n",
    "\n",
    "\n",
    "plt.plot(smooth_random_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f546a-2437-46a9-8743-4fc20f883b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "## Model and training Args\n",
    "input_size = 100 * n_neurons\n",
    "hs1 = 128\n",
    "hs2 = 256\n",
    "output_size = 1\n",
    "\n",
    "lr = 1e-4\n",
    "wd = 1\n",
    "eps = 1e-8\n",
    "epochs = 5\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate=0):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(input_size, hs1, hs2, output_size)\n",
    "accelerator = Accelerator()\n",
    "model = accelerator.prepare(model)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, eps=eps)\n",
    "loss_fn = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740730b-29ff-4de5-9ff9-4d8e0275749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "best_eval_loss = np.inf\n",
    "best_eval_epoch = 0\n",
    "\n",
    "train_loss_list = []\n",
    "eval_loss_list = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # train epoch\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch = move_batch_to_device(batch, accelerator.device)\n",
    "        spikes_flat = batch['spikes_data'].reshape(batch['spikes_data'].shape[0], -1)\n",
    "        preds = model(spikes_flat)\n",
    "        tgt = torch.tensor(smooth_random_series[batch['target'].detach().cpu().numpy().astype(np.int32)], device=preds.device, dtype=torch.float32)\n",
    "        loss = loss_fn(preds, tgt.unsqueeze(1)).sum()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        train_examples += tgt.shape[0]\n",
    "\n",
    "    train_loss /= train_examples\n",
    "    print(f\"Epoch {epoch} training loss: {train_loss}\")\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_examples = 0\n",
    "    gt_list, pred_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = move_batch_to_device(batch, accelerator.device)\n",
    "            spikes_flat = batch['spikes_data'].reshape(batch['spikes_data'].shape[0], -1)\n",
    "            preds = model(spikes_flat)\n",
    "            tgt = torch.tensor(smooth_random_series[batch['target'].detach().cpu().numpy().astype(np.int32)], device=preds.device, dtype=torch.float32)\n",
    "            loss = loss_fn(preds, tgt.unsqueeze(1)).sum()\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            eval_examples += tgt.shape[0]\n",
    "            pred_list.append(preds)\n",
    "            gt_list.append(tgt)\n",
    "            \n",
    "    gt = torch.cat(gt_list, dim=0).detach().cpu().numpy()\n",
    "    preds = torch.cat(pred_list, dim=0).detach().cpu().numpy()\n",
    "\n",
    "    gt = [x for t, x in sorted(zip(batch['target'].detach().cpu().numpy().astype(np.int32), gt))]\n",
    "    preds = [x for t, x in sorted(zip(batch['target'].detach().cpu().numpy().astype(np.int32), preds))]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(gt)\n",
    "    plt.plot(preds)\n",
    "    \n",
    "    \n",
    "    eval_loss /= eval_examples\n",
    "    if eval_loss < best_eval_loss:\n",
    "        best_eval_loss = eval_loss\n",
    "        best_eval_epoch = epoch\n",
    "        gt_best = gt\n",
    "        pred_best = preds\n",
    "        \n",
    "    print(f\"Epoch {epoch} eval loss: {eval_loss}\")\n",
    "    eval_loss_list.append(eval_loss)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(gt_best)\n",
    "plt.plot(pred_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5549ca00-0f0e-45ca-b257-8e2a37612b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
