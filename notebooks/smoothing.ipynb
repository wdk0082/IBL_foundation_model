{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee06357-bfcf-472e-9cc7-e76c068495ec",
   "metadata": {},
   "source": [
    "### Direct Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e06f19f4-d4f6-4a2e-be6d-0419902b0664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/zwang34/miniconda3/envs/ibl-fm/lib/python310.zip', '/home/zwang34/miniconda3/envs/ibl-fm/lib/python3.10', '/home/zwang34/miniconda3/envs/ibl-fm/lib/python3.10/lib-dynload', '', '/home/zwang34/miniconda3/envs/ibl-fm/lib/python3.10/site-packages', './src']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zwang34/miniconda3/envs/ibl-fm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/zwang34/miniconda3/envs/ibl-fm/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Path\n",
    "import os, sys\n",
    "os.chdir('/home/zwang34/IBL/iblfm_exp/IBL_foundation_model')\n",
    "sys.path.append('./src')\n",
    "print(sys.path)\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# Lib\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import numpy as np\n",
    "from loader.make_loader import make_loader\n",
    "from utils.eval_utils import bits_per_spike\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03a799f4-6d6c-4bd6-96c8-890998895684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Args\n",
    "EID = '671c7ea7-6726-4fbe-adeb-f89c2c8e489b'\n",
    "kernel_sigma_list = [1, 2, 3, 8, 16, 32]\n",
    "fr_filter = 0.1 # unit: 1 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b3e222c-4704-4755-80c5-fb3ae05b2dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset): 559\n",
      "len(dataset): 80\n",
      "len(dataset): 160\n",
      "valid neuron: 540, invalid neuron: 128\n",
      "20ms smoothing bps: 3.720978651248588, gt bps: 4.6170632896923385. Population bps: 1.7530588882369578, 2.280131022648492\n",
      "40ms smoothing bps: 3.131915272451578, gt bps: 4.608511572732067. Population bps: 1.4496777084966443, 2.280131022648492\n",
      "60ms smoothing bps: 2.835967394774532, gt bps: 4.608511572732067. Population bps: 1.3107413520144735, 2.280131022648492\n",
      "160ms smoothing bps: 2.241143864721745, gt bps: 4.608511572732067. Population bps: 1.0649939663353993, 2.280131022648492\n",
      "320ms smoothing bps: 1.8848178554006771, gt bps: 4.608511572732067. Population bps: 0.9141381802525307, 2.280131022648492\n",
      "640ms smoothing bps: 1.5774835660577733, gt bps: 4.608511572732067. Population bps: 0.7626687600713836, 2.280131022648492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\naxes.flat[0].set_ylabel('gt vs. gt bps')\\naxes.flat[3].set_ylabel('gt vs. gt bps')\\naxes.flat[3].set_xlabel('gt vs. smoothed bps')\\naxes.flat[4].set_xlabel('gt vs. smoothed bps')\\naxes.flat[5].set_xlabel('gt vs. smoothed bps')\\nfig.colorbar(scatter)\\nplt.savefig('smoothing_direct.png')\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prepare data\n",
    "dataset = load_dataset(f'neurofm123/{EID}_aligned', cache_dir='/expanse/lustre/scratch/zwang34/temp_project/iTransformer/checkpoints/datasets_cache')\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['val']\n",
    "test_dataset = dataset['test']\n",
    "n_neurons = len(test_dataset[0]['cluster_uuids'])\n",
    "\n",
    "train_dataloader = make_loader(\n",
    "    train_dataset,\n",
    "    target=None,\n",
    "    load_meta=True,\n",
    "    batch_size=10000,\n",
    "    pad_to_right=True,\n",
    "    pad_value=-1,\n",
    "    bin_size=0.02,\n",
    "    max_time_length=100,\n",
    "    max_space_length=n_neurons,\n",
    "    dataset_name='ibl',\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_dataloader = make_loader(\n",
    "    val_dataset,\n",
    "    target=None,\n",
    "    load_meta=True,\n",
    "    batch_size=10000,\n",
    "    pad_to_right=True,\n",
    "    pad_value=-1,\n",
    "    bin_size=0.02,\n",
    "    max_time_length=100,\n",
    "    max_space_length=n_neurons,\n",
    "    dataset_name='ibl',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_dataloader = make_loader(\n",
    "    test_dataset,\n",
    "    target=None,\n",
    "    load_meta=True,\n",
    "    batch_size=10000,\n",
    "    pad_to_right=True,\n",
    "    pad_value=-1,\n",
    "    bin_size=0.02,\n",
    "    max_time_length=100,\n",
    "    max_space_length=n_neurons,\n",
    "    dataset_name='ibl',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    train_data = batch['spikes_data'].detach().cpu().numpy()\n",
    "for batch in val_dataloader:\n",
    "    val_data = batch['spikes_data'].detach().cpu().numpy()\n",
    "for batch in test_dataloader:\n",
    "    test_data = batch['spikes_data'].detach().cpu().numpy()\n",
    "\n",
    "# Use a fr Filter\n",
    "whole_data = np.concatenate([train_data, val_data, test_data], axis=0)\n",
    "mean_fr = np.mean(whole_data, axis=(0,1)) * 50   # hz\n",
    "valid_idx = (mean_fr >= fr_filter)\n",
    "\n",
    "train_data = train_data[:, :, valid_idx]\n",
    "val_data = val_data[:, :, valid_idx]\n",
    "test_data = test_data[:, :, valid_idx]\n",
    "print(f'valid neuron: {sum(valid_idx)}, invalid neuron: {len(valid_idx)-sum(valid_idx)}')\n",
    "\n",
    "for k, kernel_sigma in enumerate(kernel_sigma_list):       \n",
    "    gt_spikes = test_data\n",
    "    smoothed_spikes = gaussian_filter1d(gt_spikes, sigma=kernel_sigma, axis=1)\n",
    "    fr_stat = np.log(np.mean(gt_spikes, axis=(0, 1))+1e-9)\n",
    "    \n",
    "    bps_smth_list = []\n",
    "    bps_gt_list = []\n",
    "    for i in range(sum(valid_idx)):\n",
    "        bps_smth_list.append(bits_per_spike(smoothed_spikes[:, :, [i]], gt_spikes[:, :, [i]]))\n",
    "        bps_gt_list.append(bits_per_spike(gt_spikes[:, :, [i]], gt_spikes[:, :, [i]]))\n",
    "    population_smth_bps = bits_per_spike(smoothed_spikes, gt_spikes)\n",
    "    population_upb_bps = bits_per_spike(gt_spikes, gt_spikes)\n",
    "\n",
    "    mean_smth_bps = np.nanmean(bps_smth_list)\n",
    "    mean_upb_bps = np.nanmean(bps_gt_list)\n",
    "    print(f'{kernel_sigma*20}ms smoothing bps: {mean_smth_bps}, gt bps: {mean_upb_bps}. Population bps: {population_smth_bps}, {population_upb_bps}')\n",
    "\n",
    "    \n",
    "    '''\n",
    "    scatter = axes.flat[k].scatter(bps_smth_list, bps_gt_list, c=fr_stat)\n",
    "    axes.flat[k].set_title(f'kernel sigma={kernel_sigma*20}ms')\n",
    "    axes.flat[k].set_xlim([-10, 15])\n",
    "    axes.flat[k].set_ylim([-10, 15])\n",
    "    axes.flat[k].plot([-10, 15], [-10, 15], c='grey')\n",
    "    '''\n",
    "\n",
    "'''\n",
    "axes.flat[0].set_ylabel('gt vs. gt bps')\n",
    "axes.flat[3].set_ylabel('gt vs. gt bps')\n",
    "axes.flat[3].set_xlabel('gt vs. smoothed bps')\n",
    "axes.flat[4].set_xlabel('gt vs. smoothed bps')\n",
    "axes.flat[5].set_xlabel('gt vs. smoothed bps')\n",
    "fig.colorbar(scatter)\n",
    "plt.savefig('smoothing_direct.png')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff50fe9-8220-4f4f-b3f9-fd940c2314d8",
   "metadata": {},
   "source": [
    "### Smoothing + Poisson GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f7d161-5e0c-4e92-8536-245086c5ba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/zwang34/miniconda3/envs/ibl-fm/lib/python310.zip', '/home/zwang34/miniconda3/envs/ibl-fm/lib/python3.10', '/home/zwang34/miniconda3/envs/ibl-fm/lib/python3.10/lib-dynload', '', '/home/zwang34/miniconda3/envs/ibl-fm/lib/python3.10/site-packages', './src', '/scratch/zwang34/job_33291653/tmpkupvmri4', './src', './src']\n"
     ]
    }
   ],
   "source": [
    "# Path\n",
    "import os, sys\n",
    "os.chdir('/home/zwang34/IBL/iblfm_exp/IBL_foundation_model')\n",
    "sys.path.append('./src')\n",
    "print(sys.path)\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# Lib\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import numpy as np\n",
    "from loader.make_loader import make_loader\n",
    "from utils.eval_utils import bits_per_spike\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2063be49-e2cc-4fbf-b126-613f62f09353",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix Args\n",
    "EID = '03d9a098-07bf-4765-88b7-85f8d8f620cc'\n",
    "randomized = False\n",
    "kernel_sigma = 2  # unit: 20 ms\n",
    "heldout_ratio = 0.1 \n",
    "fr_filter = 0.1  # unit: 1 Hz\n",
    "\n",
    "seed = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69be10fc-6e44-4cd8-bc6a-7c7944e9172c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset): 397\n",
      "len(dataset): 57\n",
      "len(dataset): 114\n",
      "held-out neurons : held-in neurons = 41 : 374\n",
      "held-out idxs: [152 521 300 424 190 354  69  82 385 314 487 114 381 303  81  89 128 501\n",
      "  65 132 442 155 313  50 459 230 200 326 367 426   8 512 333 411 444 264\n",
      " 506 161 236 499  58]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "## Prepare data\n",
    "if randomized == True:\n",
    "    dataset = load_dataset(f'neurofm123/{EID}_aligned', cache_dir='/expanse/lustre/scratch/zwang34/temp_project/iTransformer/checkpoints/datasets_cache')\n",
    "    train_dataset = dataset['train']\n",
    "    val_dataset = dataset['val']\n",
    "    test_dataset = dataset['test']\n",
    "    n_neurons = len(test_dataset[0]['cluster_uuids'])\n",
    "else:\n",
    "    dataset = load_dataset(f'neurofm123/{EID}_nonrandomized', cache_dir='/expanse/lustre/scratch/zwang34/temp_project/iTransformer/checkpoints/datasets_cache')\n",
    "    train_dataset = dataset['train']\n",
    "    val_dataset = dataset['val']\n",
    "    test_dataset = dataset['test']\n",
    "    n_neurons = len(test_dataset[0]['cluster_uuids'])\n",
    "\n",
    "train_dataloader = make_loader(\n",
    "    train_dataset,\n",
    "    target=None,\n",
    "    load_meta=True,\n",
    "    batch_size=10000,\n",
    "    pad_to_right=True,\n",
    "    pad_value=-1,\n",
    "    bin_size=0.02,\n",
    "    max_time_length=100,\n",
    "    max_space_length=n_neurons,\n",
    "    dataset_name='ibl',\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_dataloader = make_loader(\n",
    "    val_dataset,\n",
    "    target=None,\n",
    "    load_meta=True,\n",
    "    batch_size=10000,\n",
    "    pad_to_right=True,\n",
    "    pad_value=-1,\n",
    "    bin_size=0.02,\n",
    "    max_time_length=100,\n",
    "    max_space_length=n_neurons,\n",
    "    dataset_name='ibl',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_dataloader = make_loader(\n",
    "    test_dataset,\n",
    "    target=None,\n",
    "    load_meta=True,\n",
    "    batch_size=10000,\n",
    "    pad_to_right=True,\n",
    "    pad_value=-1,\n",
    "    bin_size=0.02,\n",
    "    max_time_length=100,\n",
    "    max_space_length=n_neurons,\n",
    "    dataset_name='ibl',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    train_data = batch['spikes_data'].detach().cpu().numpy()\n",
    "for batch in val_dataloader:\n",
    "    val_data = batch['spikes_data'].detach().cpu().numpy()\n",
    "for batch in test_dataloader:\n",
    "    test_data = batch['spikes_data'].detach().cpu().numpy()\n",
    "\n",
    "# Use a fr Filter\n",
    "whole_data = np.concatenate([train_data, val_data, test_data], axis=0)\n",
    "mean_fr = np.mean(whole_data, axis=(0,1)) * 50   # hz\n",
    "valid_idx = (mean_fr >= fr_filter)\n",
    "valid_idx_number = np.where(np.array(valid_idx)==1)[0]\n",
    "train_data = train_data[:, :, valid_idx]\n",
    "val_data = val_data[:, :, valid_idx]\n",
    "test_data = test_data[:, :, valid_idx]\n",
    "\n",
    "# Randomly select heldout neurons\n",
    "n_valid_neurons = train_data.shape[-1]\n",
    "heldout_idxs = np.random.choice(range(n_valid_neurons), size=int(n_valid_neurons*heldout_ratio), replace=False)\n",
    "heldout_idxs_raw = valid_idx_number[heldout_idxs]\n",
    "mask = np.ones(n_valid_neurons, dtype=bool)\n",
    "mask[heldout_idxs] = False\n",
    "\n",
    "print(f'held-out neurons : held-in neurons = {heldout_idxs.shape[0]} : {n_valid_neurons-heldout_idxs.shape[0]}')\n",
    "print(f'held-out idxs: {heldout_idxs_raw}')\n",
    "\n",
    "train_spikes_heldin = train_data[:, :, mask]\n",
    "train_spikes_heldout = train_data[:, :, ~mask]\n",
    "test_spikes_heldin = test_data[:, :, mask]\n",
    "test_spikes_heldout = test_data[:, :, ~mask]\n",
    "val_spikes_heldin = val_data[:, :, mask]\n",
    "val_spikes_heldout = val_data[:, :, ~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c10e23b-1b8d-4579-9122-4398660b7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adapt. We use Val or Test split as the eval set here.\n",
    "eval_spikes_heldin = test_spikes_heldin\n",
    "eval_spikes_heldout = test_spikes_heldout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cde4c1a-549e-4710-a166-65ebb7452002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zwang34/miniconda3/envs/ibl-fm/lib/python3.10/site-packages/sklearn/linear_model/_glm/glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "/home/zwang34/miniconda3/envs/ibl-fm/lib/python3.10/site-packages/sklearn/linear_model/_glm/glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n",
      "/home/zwang34/miniconda3/envs/ibl-fm/lib/python3.10/site-packages/sklearn/linear_model/_glm/glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    }
   ],
   "source": [
    "## Copied from NLB'21 repo\n",
    "## Define helper function for training Poisson regressor\n",
    "\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "def fit_poisson(train_input, eval_input, train_output, alpha=0.0):\n",
    "    train_pred = []\n",
    "    eval_pred = []\n",
    "    # train Poisson GLM for each output column\n",
    "    for chan in range(train_output.shape[1]):\n",
    "        pr = PoissonRegressor(alpha=alpha, max_iter=500)\n",
    "        pr.fit(train_input, train_output[:, chan])\n",
    "        train_pred.append(pr.predict(train_input))\n",
    "        eval_pred.append(pr.predict(eval_input))\n",
    "    train_pred = np.vstack(train_pred).T\n",
    "    eval_pred = np.vstack(eval_pred).T\n",
    "    return train_pred, eval_pred\n",
    "\n",
    "## Smooth spikes\n",
    "\n",
    "# Assign useful variables\n",
    "tlength = train_spikes_heldin.shape[1]\n",
    "num_train = train_spikes_heldin.shape[0]\n",
    "num_eval = eval_spikes_heldin.shape[0]\n",
    "num_heldin = train_spikes_heldin.shape[2]\n",
    "num_heldout = train_spikes_heldout.shape[2]\n",
    "\n",
    "'''\n",
    "# Smooth spikes with 40 ms std gaussian\n",
    "import scipy.signal as signal\n",
    "kern_sd_ms = 20\n",
    "kern_sd = int(round(kern_sd_ms / 20))\n",
    "window = signal.gaussian(kern_sd * 6, kern_sd, sym=True)\n",
    "window /= np.sum(window)\n",
    "filt = lambda x: np.convolve(x, window, 'same')\n",
    "\n",
    "train_spksmth_heldin = np.apply_along_axis(filt, 1, train_spikes_heldin)\n",
    "eval_spksmth_heldin = np.apply_along_axis(filt, 1, eval_spikes_heldin)\n",
    "'''\n",
    "\n",
    "# use more convenient smoothing function\n",
    "train_spksmth_heldin = gaussian_filter1d(train_spikes_heldin, kernel_sigma, axis=1)\n",
    "eval_spksmth_heldin = gaussian_filter1d(eval_spikes_heldin, kernel_sigma, axis=1)\n",
    "\n",
    "## Generate rate predictions\n",
    "\n",
    "# Reshape data to 2d for regression\n",
    "train_spksmth_heldin_s = train_spksmth_heldin.reshape(-1, train_spksmth_heldin.shape[2])\n",
    "eval_spksmth_heldin_s = eval_spksmth_heldin.reshape(-1, eval_spksmth_heldin.shape[2])\n",
    "train_spikes_heldout_s = train_spikes_heldout.reshape(-1, train_spikes_heldout.shape[2])\n",
    "\n",
    "# Train Poisson regressor from log of held-in smoothed spikes to held-out spikes\n",
    "train_spksmth_heldout_s, eval_spksmth_heldout_s = fit_poisson(\n",
    "    np.log(train_spksmth_heldin_s + 1e-4), # add constant offset to prevent taking log of 0\n",
    "    np.log(eval_spksmth_heldin_s + 1e-4),\n",
    "    train_spikes_heldout_s,\n",
    "    alpha=0.1,\n",
    ")\n",
    "\n",
    "# Reshape data back to the same 3d shape as the input arrays\n",
    "train_rates_heldin = train_spksmth_heldin_s.reshape((num_train, tlength, num_heldin))\n",
    "train_rates_heldout = train_spksmth_heldout_s.reshape((num_train, tlength, num_heldout))\n",
    "eval_rates_heldin = eval_spksmth_heldin_s.reshape((num_eval, tlength, num_heldin))\n",
    "eval_rates_heldout = eval_spksmth_heldout_s.reshape((num_eval, tlength, num_heldout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a84bd9-3293-48ec-a07f-20c38105e15d",
   "metadata": {},
   "source": [
    "#### Note\n",
    "- held-in neuron rates in both train and eval split are obtained by directly smoothing.\n",
    "- held-out neuron rates in both train and eval split are obtained by GLM regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9c3b061-0514-47f0-bfba-bcb8d9dca8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Population) train_heldout_bps: 0.3650479659630438, eval_heldout_bps: 0.22753139228508354\n",
      "(Neuron average) train_heldout_bps: 0.8325431212139421, eval_heldout_bps: 0.42697066213162116\n"
     ]
    }
   ],
   "source": [
    "## Population Bps\n",
    "train_bps = bits_per_spike(train_rates_heldout, train_spikes_heldout)\n",
    "eval_bps = bits_per_spike(eval_rates_heldout, eval_spikes_heldout)\n",
    "## Neuron Average Bps\n",
    "train_bps_list = []\n",
    "eval_bps_list = []\n",
    "for i in range(train_rates_heldout.shape[-1]):\n",
    "    train_bps_list.append(bits_per_spike(train_rates_heldout[:, :, [i]], train_spikes_heldout[:, :, [i]]))\n",
    "    eval_bps_list.append(bits_per_spike(eval_rates_heldout[:, :, [i]], eval_spikes_heldout[:, :, [i]]))\n",
    "\n",
    "print(f\"(Population) train_heldout_bps: {train_bps}, eval_heldout_bps: {eval_bps}\")\n",
    "print(f'(Neuron average) train_heldout_bps: {np.mean(train_bps_list)}, eval_heldout_bps: {np.mean(eval_bps_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613f2fc-5600-4c5f-b33c-b5e58dc4040a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
